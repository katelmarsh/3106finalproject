---
title: "project_3"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## API Calls 

```{r} 
library(httr)
library(jsonlite)

k <- 5000
data <- list(rep(NA,k))
for(i in 0:k){
  j <- i*20
  get_url <- paste0("https://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2019-03-15:2021-03-15&api-key=wgJfZeGJBfKXqHXLC7MKYIhusdKGy2y5&offset=",j)
  nyt_data <- GET(get_url)
  nyt_data <- content(nyt_data)
  if(nyt_data[[3]] == FALSE){
    break
  }
  for(n in 1:length(nyt_data)){
    data[n+j] <- nyt_data[n]
  }
  Sys.sleep(10)
}

nyt_data$status_code

dim(data)

write_json(data, "nyt_movie_reviews_covid.json")
data <- read.csv("nyt_reviews_covid.csv")


```

```{r}
library(quanteda)
library(qdap)
library(stringr)
library(knitr)
library(tm)

summary <- data$summary 

summary_cleaned <- c()
for (i in seq_along(summary)){
    without_stopwords <- rm_stopwords(
    summary[i],
    stopwords = qdapDictionaries::OnixTxtRetToolkitSWL1, 
    unlist = FALSE,
    separate = TRUE,
    strip = TRUE,
    unique = FALSE,
    char.keep = NULL,
    lower.case = TRUE,
    names = FALSE,
    ignore.case = TRUE,
    apostrophe.remove = FALSE
  )
  text <- unlist(without_stopwords)
  text <- str_replace_all(text, pattern = '\n', replacement = "") # Remove \n
  text <- str_replace_all(text, pattern = '\u0092', replacement = "'") #Replace with quote
  text <- str_replace_all(text, pattern = '\u0091', replacement = "'") #Replace with quote
  text <- str_replace_all(text, pattern = '[:punct:]', replacement = "") #Remove punctuation
  text <- str_replace_all(text, pattern = '$', replacement = "") #Remove punctuation
  text <- str_replace_all(text, pattern = '-', replacement = "") #Remove punctuation
  text <- str_replace_all(text, pattern = 'Â·', replacement = "") #Remove punctuation
  text <- str_replace_all(text, pattern="([8+[0-9]{4}+])", replacement="") # removing all 8000-8999 ASCII errors 
  text <- str_replace_all(text, pattern = 'M&#233;',replacement = "M") # fix broken M 
  text <- stemDocument(text, language = "english") # stemming words! 
  without_stopwords <- as.list(text)
  combine <- paste0(without_stopwords, collapse = " ")
  summary_cleaned <- append(summary_cleaned, combine) 
}
s_cleaned <- quanteda::dfm(summary_cleaned, verbose = FALSE)

dim(s_cleaned)
target_freq <- as.numeric(s_cleaned)
freqs_mat <- as.matrix(s_cleaned)
doc_freq <- apply(freqs_mat,2,function(x) mean(x>0))
idf <- 1/doc_freq
idf_mat <- rep(idf,nrow(freqs_mat), byrow = TRUE, nrow = nrow(freqs_mat))
tf_idf <- freqs_mat * idf_mat

```

```{r}
# get the top k tokens with the highest tf-idf value
k <- 15
i <- 1
keyword_lists <- data.frame(matrix(NA, nrow = nrow(tf_idf), ncol = k))
for (i in 1:nrow(tf_idf)){
  keyword_lists[i,] <- names(tf_idf[i,][order(tf_idf[i,],decreasing = TRUE)[1:k]])
}
head(keyword_lists)
```

```{r}
# TF-IDF and cosine similarity
# document clustering! with https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html

# changing cosine similarity to a distance 
csim <- tf_idf / sqrt(rowSums(tf_idf * tf_idf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)

#h clust 
k <- 36
hc <- hclust(cdist, "ward.D2")
plot(hc)
clustering <- cutree(hc, h=2.5)
plot(hc, main = "'Complete' Hierarchical Clustering of Movie Review Summaries TF-IDF",
     xlab = "Cosine Similarity as Distance")
rect.hclust(hc, k, border = "#e56b6f")
p_words <- colSums(freqs_mat) / sum(freqs_mat)
cluster_words <- lapply(unique(clustering), function(x){
  rows <- freqs_mat[clustering == x,]
  # for memory's sake, drop all words that don't appear in the cluster
  rows <- rows[ , colSums(rows) > 0 ]
  colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
```

```{r visualization}
library(formattable)
cluster_summary <- data.frame(cluster = unique(clustering),
                              size = as.numeric(table(clustering)),
                              top_words = sapply(cluster_words, function(d){
                                paste(
                                  names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                  collapse = ", ")
                              }),
                              stringsAsFactors = FALSE)
formattable(cluster_summary)

# troubleshooting 
#which(grepl("and", data$summary))


```


```{r}
ratings <- read.csv("~/Downloads/title.ratings.tsv", sep = "\t")
require(data.table)
basics <- fread("~/Downloads/title.basics.tsv", sep = "\t")

merged <- merge(ratings, basics, by="tconst")

merged <- merged[which(merged$startYear >= 2019),] 
merged <- merged[which(merged$startYear <= 2021),] 
merged <- merged[which(merged$titleType == 'movie' | merged$titleType == 'tvmovie'),] 
write.csv(merged, "imdb_data.csv")
dim(merged)
#16557 rows lol 

final_merge <- merge(merged, data, by.x="primaryTitle", by.y="title")

# data cleaning, corre
opening_dates <- rep(NA, length(rownames(final_merge)))
for(i in seq_along(rownames(final_merge))){
  opening_dates[i] <- strftime(strsplit(as.character(final_merge$opening_date[i]),split="T")[[1]][1], format='%Y-%m-%d')
} 
final_merge$opening_date <- as.Date(unlist(opening_dates))
final_merge <- final_merge[!which(final_merge$opening_date <= 2019-03-15),] 

write.csv(final_merge, "nyt_and_imdb_data.csv")

mean(which(final_merge$averageRating > quantile(final_merge$averageRating,prob=1-21.85/100, na.rm = TRUE))
     %in% which(final_merge$critics_pick == 1))

genres <- list()
for(i in seq_along(rownames(final_merge))){
  genres[5] <- as.list(strsplit(as.character(final_merge$genres[5]),split=",")[[1]])
} 
#table(genres)

# merge by social secruity data by first name 

```

Interesting stats for the project pre-covid analysis: 
- 21.85% of movies in this time period are "critics picks" 
- the average movie rating is a 6.37
- the top 3rd quartile is 7.1 movie rating 
- only .3536 in the top quartile of IMDB ratings also have a critics pick 
- at the same time, there is a significant correlation between the averageRating and the critics_pick 


```{r dates}
range(final_merge$opening_date) 

# making pre and post covid variable 
for(i in 1:length(rownames(final_merge))){
  if (final_merge$opening_date[i] >= "2020-03-15"){
    final_merge$covid[i] <- 1
  }
  else final_merge$covid[i] <- 0 
}

```


```{r}
final_merge <- fread("~/Downloads/nyt_and_imdb_data.csv")

final_merge$runtimeMinutes <- as.numeric(final_merge$runtimeMinutes)

hist(final_merge$opening_date, "days") # super interesting! 
hist(final_merge$opening_date, "months") # super interesting! 

ols <- glm(critics_pick ~ opening_date, data=final_merge, family="binomial")
summary(ols)
# high p value 

ols <- lm(averageRating ~ opening_date, data=final_merge)
summary(ols)
# high p value 

hist(final_merge$numVotes)
hist(final_merge$runtimeMinutes)

ols <- glm(critics_pick ~ runtimeMinutes + averageRating + numVotes, data=final_merge, family="binomial")
summary(ols)
# critics pick and average rating have a significant relatinship!! 

ols <- lm(averageRating ~ runtimeMinutes + numVotes + sent_sd + sent_sum, data=final_merge)
summary(ols)
# runtime, numVotes, critics pick, runtime, and sent sd have a significant relationship 

ols <- glm(covid ~ runtimeMinutes + averageRating + numVotes + critics_pick + sent_mean + sent_sd + sent_sum, data=final_merge, family="binomial")
summary(ols)
# highly correlated with the number of votes, sent_mean, and sent_sum 
plot(ols)

plot(final_merge$sent_sd, final_merge$averageRating) 
hist(final_merge$sent_sum)

cor(final_merge$averageRating, final_merge$numVotes, na.rm=TRUE)

```
```{r}
# should we do PCA? 






```

```{r visualization}
library(syuzhet)
for(i in 1:length(rownames(final_merge))){
  text <- get_tokens(final_merge$summary[i])
  syuzhet_vector <- get_sentiment(text, method="syuzhet")
  final_merge$sent_mean[i] <- mean(syuzhet_vector)
  final_merge$sent_sd[i] <- sd(syuzhet_vector)
  final_merge$sent_sum[i] <- sum(syuzhet_vector)
}
tail(final_merge)

# one data issue is that there are 267 summaries that do not have any positive or negative words in the dataset and therefore have a mean, sd, and sum of 0 


final_merge[which(final_merge$sent_sum ==max(final_merge$sent_sum)),] # most positive 
final_merge[which(final_merge$sent_sum ==min(final_merge$sent_sum)),] # most negative 

```

```{r}
library(genderdata)
for (i in 1:length(nyt_reviews$author)) {
  gender <- gender(nyt_reviews$first_name)$gender
  nyt_reviews$gender[i] <- gender
  i <- i + 1
}


```

